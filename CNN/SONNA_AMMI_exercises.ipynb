{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SONNA_AMMI_exercises.ipynb","version":"0.3.2","provenance":[{"file_id":"https://gist.github.com/bartvm/872d6893d252cebaa06b96bb3c4e60bd#file-ammi-exercises-ipynb","timestamp":1552979364003}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"colab_type":"text","id":"6pgutEZ_nKdM"},"cell_type":"markdown","source":["# Automatic differentiation tutorial\n"]},{"metadata":{"id":"cOaQ3BHxp_h5","colab_type":"text"},"cell_type":"markdown","source":["This hidden block defines some utility functions and imports packages. You should run it before you execute any of the cells below.\n","\n","> Indented block\n","\n"]},{"metadata":{"colab_type":"code","cellView":"form","id":"i7l9RJ2VnKdX","colab":{}},"cell_type":"code","source":["#@title\n","import math\n","import unittest\n","\n","def forward_primitive(func):\n","  \"\"\"Overload primitives.\n","\n","  This makes sure that if a function is called on a regular\n","  value, the original primitive is called. However, if it\n","  is called on a dual number, the dual number primitive is\n","  called instead.\n","\n","  \"\"\"\n","  def wrap(dual_func):\n","    def overloaded_func(x):\n","      if isinstance(x, Dual):\n","        return dual_func(x)\n","      else:\n","        return func(x)\n","    return overloaded_func\n","  return wrap\n","\n","def backward_primitive(func):\n","  \"\"\"Overload primitives.\n","\n","  This makes sure that if a function is called on a regular\n","  value, the original primitive is called. However, if it\n","  is called on a Float, the Float number primitive is\n","  called instead.\n","\n","  \"\"\"\n","  def wrap(float_func):\n","    def overloaded_func(x):\n","      if isinstance(x, Float):\n","        return float_func(x)\n","      else:\n","        return func(x)\n","    return overloaded_func\n","  return wrap"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VngKQw1Mp-5u","colab_type":"text"},"cell_type":"markdown","source":["\n","# Forward mode using operator overloading\n","\n","As discussed in class, forward mode is commonly implemented by replacing numbers with dual numbers of the form $a + b\\varepsilon$ where $\\varepsilon^2 = 0$. The first component represents the intermediate value of the primal computation, and the second component represents the partial derivative at the intermediate value with respect to an input."]},{"metadata":{"colab_type":"text","id":"qxRNT-JCnKdP"},"cell_type":"markdown","source":["## Overloading arithmetic operators\n","\n","We will begin by implementing a simple version of dual numbers in Python. Python supports operator overloading of arithmetic operators for arbitrary objects by defining special functions of the form `__add__`, `__mul__`, etc. Consider the following example."]},{"metadata":{"colab_type":"code","cellView":"both","id":"_jx0K2VMnKdQ","colab":{}},"cell_type":"code","source":["class Dual:\n","  \"\"\"A dual number of the form a + bε where ε² = 0.\"\"\"\n","  def __init__(self, a, b):\n","    self.a = a\n","    self.b = b\n","\n","  def __str__(self):\n","    return f\"{self.a} + {self.b}ε\"\n","    \n","  def __add__(self, other):\n","    \"\"\"Addition of dual numbers.\n","    \n","    Note that (a + bε) + (c + dε) = (a + c) + (b + d)ε.\n","    \n","    \"\"\"\n","    return Dual(self.a + other.a, self.b + other.b)\n","\n","  def __mul__(self, other):\n","    \"\"\"(a + bε)(c + dε).\"\"\"\n","    \n","    return Dual(self.a * other.a , self.a * other.b + self.b * other.a)\n","  \n","\n","  def __truediv__(self, other):\n","    \"\"\"(a + bε) / (c + dε).\"\"\"\n","    \n","    return Dual(self.a/other.a , (self.b / other.a) -(self.a * other.b)/(other.a**2))\n","    \n","\n","  def __neg__(self):\n","    \"\"\"-(a + bε).\"\"\"\n","    return Dual(-self.a, -self.b)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"dbf8ee9c-2fcf-4271-bbd9-21952ef13c58","id":"2U7xXs-lnKdT","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1552989826579,"user_tz":-120,"elapsed":1313,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["# We can add two scalars\n","x, y = 0, 2\n","print(x + y)\n","\n","# And we can add two dual numbers the same way\n","x, y = Dual(0, 1), Dual(2, 2)\n","print(x + y)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["2\n","2 + 3ε\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"iqqCYXi-nKdW"},"cell_type":"markdown","source":["Note that the arithmetic of dual numbers obey the rules of differentiation for the second component. For example, if $w = yz$ then the product rule tells us that $\\frac{dw}{dx} = \\frac{dy}{dx}z + y\\frac{dz}{dx}$. Similarly, $(a + b\\varepsilon)(c + d\\varepsilon) = ac + (ad + bc)\\varepsilon + bd\\varepsilon^2= ac + (ad + bc)\\varepsilon$.\n"]},{"metadata":{"id":"xps9vJTLoH1f","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise\n","\n","Complete the class accordingly by using the arithmetic of dual numbers or the rules of differentiation."]},{"metadata":{"id":"RCmWr6tNocUK","colab_type":"text"},"cell_type":"markdown","source":["**Solution**:"]},{"metadata":{"id":"cBPwuKAfnbnZ","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title\n","class Dual:\n","  \"\"\"A dual number of the form a + bε where ε² = 0.\"\"\"\n","  def __init__(self, a, b):\n","    self.a = a\n","    self.b = b\n","\n","  def __str__(self):\n","    return f\"{self.a} + {self.b}ε\"\n","    \n","  def __add__(self, other):\n","    \"\"\"Addition of dual numbers.\n","    \n","    Note that (a + bε) + (c + dε) = (a + c) + (b + d)ε.\n","    \n","    \"\"\"\n","    return Dual(self.a + other.a, self.b + other.b)\n","\n","  def __mul__(self, other):\n","    \"\"\"(a + bε)(c + dε).\"\"\"\n","    return Dual(self.a * other.a, self.a * other.b + self.b * other.a)\n","\n","  def __truediv__(self, other):\n","    \"\"\"(a + bε) / (c + dε).\"\"\"\n","    return Dual(self.a / other.a, (self.b * other.a - self.a * other.b) / other.a ** 2)\n","\n","  def __neg__(self):\n","    \"\"\"-(a + bε).\"\"\"\n","    return Dual(-self.a, -self.b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AASnmyjkoiMd","colab_type":"text"},"cell_type":"markdown","source":["**Unit tests**: run the following cell to check your implementation."]},{"metadata":{"id":"T9qcelLTonXa","colab_type":"code","cellView":"form","outputId":"d47293e5-6cfe-4652-f711-5befbf74d4cb","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1552989831157,"user_tz":-120,"elapsed":1473,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["#@title\n","class TestDual(unittest.TestCase):\n","  def test_add(self):\n","    x, y = Dual(0, 1), Dual(2, 2)\n","    z = x + y\n","    self.assertEqual(z.a, 2)\n","    self.assertEqual(z.b, 3)\n","\n","  def test_mul(self):\n","    x, y = Dual(0, 1), Dual(2, 2)\n","    z = x * y\n","    self.assertEqual(z.a, x.a * y.a)\n","    self.assertEqual(z.b, x.a * y.b + x.b * y.a)\n","\n","  def test_truediv(self):\n","    x, y = Dual(0, 1), Dual(2, 2)\n","    z = x / y\n","    self.assertEqual(z.a, x.a / y.a)\n","    self.assertEqual(z.b, (x.b * y.a - x.a * y.b) / y.a ** 2)\n","\n","  def test_neg(self):\n","    y = Dual(2, 2)\n","    z = -y\n","    self.assertEqual(z.a, -y.a)\n","    self.assertEqual(z.b, -y.b)\n","\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestDual)\n","_ = unittest.TextTestRunner(verbosity=2).run(suite)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["test_add (__main__.TestDual) ... ok\n","test_mul (__main__.TestDual) ... ok\n","test_neg (__main__.TestDual) ... ok\n","test_truediv (__main__.TestDual) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 4 tests in 0.016s\n","\n","OK\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"sLbCPQ1znKdX"},"cell_type":"markdown","source":["## Overloading primitives\n","\n","Similarly, we can define mathematical functions which operate on dual numbers. Let's write a logarithm function which works on dual numbers."]},{"metadata":{"colab_type":"code","id":"vdEkplH4nKda","colab":{}},"cell_type":"code","source":["@forward_primitive(math.log)\n","def log(x):\n","  return Dual(math.log(x.a), x.b / x.a)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QgE6HhuwtfMl","colab_type":"code","outputId":"35e79f9c-9d70-472c-bcd5-ffad40019489","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989834123,"user_tz":-120,"elapsed":1774,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["x = Dual(2, 1)\n","print(log(x))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.6931471805599453 + 0.5ε\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"PdgeXNYKnKdc"},"cell_type":"markdown","source":["#### Exercise\n","\n","Implement the overloaded exponential operator similarly to how the logarithm operator was implemented."]},{"metadata":{"colab_type":"code","cellView":"both","id":"658seqKvnKdd","colab":{}},"cell_type":"code","source":["@forward_primitive(math.exp)\n","def exp(x):\n","  \n","  return Dual(math.exp(x.a),math.exp(a)*x.b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-Zbm94MVT8UZ","colab_type":"text"},"cell_type":"markdown","source":["**Solution**:"]},{"metadata":{"id":"6B7RHSEST7Ng","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title\n","@forward_primitive(math.exp)\n","def exp(x):\n","  exp_a = math.exp(x.a)\n","  return Dual(exp_a, x.b * exp_a)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VF86WzKgUIV0","colab_type":"text"},"cell_type":"markdown","source":["**Unit tests**: run the following cell to check your implementation."]},{"metadata":{"id":"hOvcpTIPUJOx","colab_type":"code","cellView":"form","outputId":"859bc7d7-612e-456c-c5aa-f28622bdfb55","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1552989843363,"user_tz":-120,"elapsed":1498,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["#@title\n","class TestPrimitives(unittest.TestCase):\n","  def test_log(self):\n","    x = Dual(2, 1)\n","    y = log(x)\n","    self.assertAlmostEqual(y.a, math.log(2))\n","    self.assertAlmostEqual(y.b, 0.5)\n","\n","  def test_exp(self):\n","    x = Dual(2, 1)\n","    y = exp(x)\n","    self.assertEqual(y.a, math.exp(2))\n","    self.assertEqual(y.b, math.exp(2))\n","\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestPrimitives)\n","_ = unittest.TextTestRunner(verbosity=2).run(suite)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["test_exp (__main__.TestPrimitives) ... ok\n","test_log (__main__.TestPrimitives) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 2 tests in 0.008s\n","\n","OK\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"od0BHM1EnKde"},"cell_type":"markdown","source":["## Testing it out\n","\n"]},{"metadata":{"id":"2blGIA4EVx6O","colab_type":"text"},"cell_type":"markdown","source":["We now have all the pieces to calculate the derivative of a weighted logistic function, $f(x) = 1 / (1 + e^{-tx})$. To calculate the derivative with respect to $x$, we have to set the initial dual values of $x$ and $t$ appropriately.\n"]},{"metadata":{"id":"g2-66xAiV8po","colab_type":"code","colab":{}},"cell_type":"code","source":["def logistic(x, t, one):\n","  return one / (one + exp(-(t * x)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bc-a7zPQWDRQ","colab_type":"code","outputId":"281dfad4-84a1-45ca-c7b1-91490954b11e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989852871,"user_tz":-120,"elapsed":1820,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["# The regular output\n","y = logistic(1, 2, 1)\n","print(f\"y = {y:.4}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["y = 0.8808\n"],"name":"stdout"}]},{"metadata":{"id":"NJhxk3C8VzCW","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise\n","\n","Set the correct `b` values of `Dual` instances for $x$ and $t$."]},{"metadata":{"colab_type":"code","id":"6fMkz_uqnKdf","colab":{}},"cell_type":"code","source":["# The derivative\n","one = Dual(1, 0)\n","x = Dual(1, 1)\n","t = Dual(2, 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jCPqATtLW6Ju","colab_type":"text"},"cell_type":"markdown","source":["**Solution**:"]},{"metadata":{"id":"rWD1WYszWcf7","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title\n","one = Dual(1, 0)\n","x = Dual(1, 1)\n","t = Dual(2, 0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-zpcFODLXplE","colab_type":"text"},"cell_type":"markdown","source":["**Unit tests**: run the following cell to check your implementation."]},{"metadata":{"id":"aJh6TAo7X6E4","colab_type":"code","cellView":"both","outputId":"dac7dfc8-203c-43e8-9c45-a16aac4979ff","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1552989906233,"user_tz":-120,"elapsed":1411,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["#@title\n","class TestLogistic(unittest.TestCase):\n","  def setUp(self):\n","    self.one = one\n","    self.x = x\n","    self.t = t\n","\n","  def test_derivative(self):\n","    y = logistic(self.x, self.t, self.one)\n","    self.assertAlmostEqual(y.b, y.a * (1 - y.a) * self.t.a)\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestLogistic)\n","_ = unittest.TextTestRunner(verbosity=2).run(suite)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["test_derivative (__main__.TestLogistic) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.002s\n","\n","OK\n"],"name":"stderr"}]},{"metadata":{"id":"HHarlNp5ZJev","colab_type":"text"},"cell_type":"markdown","source":["A passing test indicates that our implementation is correct:"]},{"metadata":{"id":"Hb3J0skuWyyA","colab_type":"code","outputId":"e005ec10-839d-4bc8-bb33-f839b448e91d","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989908901,"user_tz":-120,"elapsed":999,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["y = logistic(x, t, one)\n","dydx = y.b\n","print(f\"dy/dx = {dydx:.4}\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["dy/dx = 0.21\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"7uC8-gn1nKdk"},"cell_type":"markdown","source":["However, the test was written by deriving the correct expression for the gradient, which for more complex functions may be very tedious. Another way to verify our implementation is by comparing it with simple finite differences where we approximate $f'(x) \\approx \\frac{f(x + \\Delta) - f(x - \\Delta)}{2\\Delta}$."]},{"metadata":{"id":"I7fIxLRraDnw","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise\n","\n","Confirm that your implementation is correct by checking if the finite differences approximation is close to the derivative calculated using automatic differentiation."]},{"metadata":{"id":"Sh3vUC4uECd-","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"edf12e5e-794c-42b6-c8ac-6b5eb2d78775","cellView":"both","id":"aBC4cGyhnKdl","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989912321,"user_tz":-120,"elapsed":1012,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["Delta = 0.01\n","dydx = (logistic(x.a + Delta, t.a, one.a)-logistic(x.a - Delta, t.a, one.a))/(2*Delta) # Replace this with the finite difference approximation\n","print(f\"dy/dx = {dydx:.4}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["dy/dx = 0.21\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"yvnYjoHvnKdq"},"cell_type":"markdown","source":["# Reverse mode using operator overloading\n","\n","As discussed in class, forward mode scales with the number of inputs. If we want the derivative of the weighted logistic function with respect to both $x$ and $t$, we would have to execute the function twice, once with $x = 1 + \\varepsilon$ and once with $t = 2 + \\varepsilon$.\n","\n","We will avoid this overhead by implementing reverse mode automatic differentiation. We will start by using something similar to dual numbers, replacing each variable with an object that holds both the intermediate value and its partial derivative.\n","\n","However, unlike in forward mode, we won't calculate the partial derivative straight away. Instead, we will log the computation on a tape. The partial derivative will then be calculated during the backward pass, when the tape is walked in reverse."]},{"metadata":{"colab_type":"code","id":"FYW_VGAhnKds","colab":{}},"cell_type":"code","source":["from typing import NamedTuple, Callable, Sequence\n","\n","# The tape is simply a list\n","TAPE = []\n","\n","class TapeEntry(NamedTuple):\n","  \"\"\"On the tape we log the called function, its inputs, and its outputs.\"\"\"\n","  function: Callable[..., float]\n","  inputs: Sequence\n","  output: float  # For simplicity, we assume a single output"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"L8oLFYoUnKdu","colab":{}},"cell_type":"code","source":["\n","\n","import operator\n","\n","class Float:\n","  \"\"\"Scalar values that also hold their partial derivative.\"\"\"\n","  def __init__(self, value):\n","      self.value = value\n","      self.grad = 0.0\n","\n","  def __add__(self, other):\n","    # Add two numbers and create a new Float\n","    out = Float(self.value + other.value)\n","    \n","    # Write to the tape that this addition was performed\n","    TAPE.append(TapeEntry(operator.add, (self, other), out))\n","\n","    return out\n","\n","  def __repr__(self):\n","    return repr(self.value)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"31bb048b-e94b-4fac-99a9-d54f554e0b5b","id":"zZUSeox9nKdx","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1552989916623,"user_tz":-120,"elapsed":838,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["# We can now generate numbers that have an associated gradient\n","x = Float(2.0)\n","print(x + x)\n","x.grad += 1\n","print(x.grad)\n","\n","# And when we add two numbers, this was logged on the tape\n","print(TAPE)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["4.0\n","1.0\n","[TapeEntry(function=<built-in function add>, inputs=(2.0, 2.0), output=4.0)]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"9gLIKDd9nKd0"},"cell_type":"markdown","source":["We now need to define the gradient for each operation, which will be performed during the backward pass. Consider the following example for addition:"]},{"metadata":{"colab_type":"code","id":"QkQ8_8rZnKd1","colab":{}},"cell_type":"code","source":["# For each primitive, we have to say what its derivative is\n","GRADS = {}\n","\n","def add_grad(x, y, z):\n","  # Note that dL/dx = dL/dz * dz/dx = dL/dz\n","  # Since z = x + y, dz/dx = 1, so dL/dx = dL/dz\n","  x.grad += z.grad\n","  # Similarly for dL/dy\n","  y.grad += z.grad\n","\n","GRADS[operator.add] = add_grad"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"uh8W9CunnKd2"},"cell_type":"markdown","source":["Each operation is now logged to a tape, and for each operation we know what gradient computations to perform during the backward pass. The only thing left to do is then to implement the backward pass by walking the tape in reverse."]},{"metadata":{"id":"E5ZMC6u4cAsx","colab_type":"text"},"cell_type":"markdown","source":["#### Exercise\n","\n","Walk the tape in reverse. For each operation on the tape, call its corresponding gradient with the inputs and output."]},{"metadata":{"colab_type":"code","id":"rR2w9YdInKd3","colab":{}},"cell_type":"code","source":["def grad(f):\n","  def df(*args, **kwargs):\n","    # Begin with an empty tape\n","    TAPE.clear()\n","\n","    # Now call the original function, which will write to the tape\n","    out = f(*args, **kwargs)\n","\n","    # The initial gradient of the output is 1\n","    out.grad = 1\n","\n","    for entry in reversed(TAPE):\n","      grad_function= GRADS[entry.function]\n","      #pass\n","      # TODO: Call the grad_function with *inputs and output.\n","      #pass\n","      grad_function(*entry.inputs,entry.output)\n","\n","    # We return the gradient with respect to each of the input arguments\n","    return tuple(arg.grad if isinstance(arg, Float) else None for arg in args)\n","  return df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ges6D3HWcJUc","colab_type":"text"},"cell_type":"markdown","source":["**Solution**:"]},{"metadata":{"id":"SNp1x1nscLf7","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title\n","def grad(f):\n","  def df(*args, **kwargs):\n","    # Begin with an empty tape\n","    TAPE.clear()\n","\n","    # Now call the original function, which will write to the tape\n","    out = f(*args, **kwargs)\n","\n","    # The initial gradient of the output is 1\n","    out.grad = 1\n","\n","    for entry in reversed(TAPE):\n","      grad_function = GRADS[entry.function]\n","      grad_function(*entry.inputs, entry.output)\n","\n","    # We return the gradient with respect to each of the input arguments\n","    return tuple(arg.grad if isinstance(arg, Float) else None for arg in args)\n","  return df"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vBRByXHJclT-"},"cell_type":"markdown","source":["**Unit tests**: run the following cell to check your implementation."]},{"metadata":{"colab_type":"code","cellView":"form","outputId":"fea876e2-0f86-4f88-af8a-436810db5d9a","id":"7yB9-QajclT_","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1552989924053,"user_tz":-120,"elapsed":655,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["#@title\n","class TestGrad(unittest.TestCase):\n","  def test_derivative(self):\n","    def _f(x, y):\n","      return x + y + y\n","    dx, dy = grad(_f)(Float(2), Float(3))\n","    self.assertAlmostEqual(dx, 1.0)\n","    self.assertAlmostEqual(dy, 2.0)\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestGrad)\n","_ = unittest.TextTestRunner(verbosity=2).run(suite)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["test_derivative (__main__.TestGrad) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.002s\n","\n","OK\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"SqnX-ErYnKd5"},"cell_type":"markdown","source":["Let's try our validated implementation on a simple example. For the following example, we want to calculate the gradient with respect to both $x$ and $y$."]},{"metadata":{"colab_type":"code","outputId":"3e495e4b-b12e-46ab-e70c-3eb40e6359c0","id":"d5LpTjMVnKd6","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989929461,"user_tz":-120,"elapsed":1015,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["def f(x, y):\n","  return x + y + y\n","\n","dx, dy = grad(f)(Float(2), Float(3))\n","print(f\"dx = {dx:.4}, dy = {dy:.4}\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["dx = 1.0, dy = 2.0\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"FI40PkIBnKd9"},"cell_type":"markdown","source":["Below there is a version of the `Float` class which traces division, multiplication, and negation as well. It also introduces an exponential function which operates on our special `Float` values. The only thing that is missing is their actual gradients."]},{"metadata":{"colab_type":"code","id":"sNWsEkcgnKeA","colab":{}},"cell_type":"code","source":["def trace(primitive):\n","  \"\"\"Overload a function.\n","\n","  This function takes a primitive, and returns a new\n","  overloaded function which applies the primitive but\n","  also logs the application of the primitive to the tape.\n","\n","  This is just a programmatic way of writing the __add__\n","  function from the previous Float class.\n","\n","  \"\"\"\n","  def overloaded_primitive(*args):\n","    out = Float(primitive(*(arg.value for arg in args)))\n","    TAPE.append(TapeEntry(primitive, args, out))\n","    return out\n","  return overloaded_primitive\n","\n","class Float:\n","  def __init__(self, value):\n","      self.value = value\n","      self.grad = 0.0\n","\n","  # Overloaded operators\n","  __add__ = trace(operator.add)\n","  __truediv__ = trace(operator.truediv)\n","  __mul__ = trace(operator.mul)\n","  __neg__ = trace(operator.neg)\n","\n","  def __repr__(self):\n","    return repr(self.value)\n","\n","@backward_primitive(math.exp)\n","def exp(x):\n","  out = Float(math.exp(x.value))\n","  TAPE.append(TapeEntry(math.exp, (x,), out))\n","  return out"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Tzbi8k7inKeC"},"cell_type":"markdown","source":["#### Exercise\n","\n","Implement the backwards gradients for multiplication, negation, division, and the exponential function so that you can calculate the gradient of the weighted logistic function with respect both $x$ and $t$."]},{"metadata":{"colab_type":"code","id":"OSnwVjePnKeD","colab":{}},"cell_type":"code","source":["def mul_grad(x, y, z):\n","  x.grad += z.grad * y.value# TODO: replace with the correct expression.\n","  y.grad += z.grad * x.value # TODO: replace with the correct expression.\n","\n","GRADS[operator.mul] = mul_grad\n","\n","def truediv_grad(x, y, z):\n","  x.grad += z.grad * (1/y.value)# TODO: replace with the correct expression.\n","  y.grad += -z.grad * x.value* (1/y.value**2) # TODO: replace with the correct expression.\n","\n","GRADS[operator.truediv] = truediv_grad\n","\n","def neg_grad(x, y):\n","  x.grad += -y.grad # TODO: replace with the correct expression.\n","\n","GRADS[operator.neg] = neg_grad\n","\n","def exp_grad(x, y):\n","  x.grad += y.grad * math.exp(x.value) # TODO: replace with the correct expression.\n","\n","GRADS[math.exp] = exp_grad"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y3dWlKktfZeE","colab_type":"text"},"cell_type":"markdown","source":["**Solution**:"]},{"metadata":{"id":"5TM_CBZgfKiI","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title\n","def mul_grad(x, y, z):\n","  x.grad += y.value * z.grad\n","  y.grad += x.value * z.grad\n","\n","GRADS[operator.mul] = mul_grad\n","\n","def truediv_grad(x, y, z):\n","  x.grad += z.grad / y.value\n","  y.grad += -z.grad * x.value / (y.value * y.value)\n","\n","GRADS[operator.truediv] = truediv_grad\n","\n","def neg_grad(x, y):\n","  x.grad += -y.grad\n","\n","GRADS[operator.neg] = neg_grad\n","\n","def exp_grad(x, y):\n","  x.grad += y.grad * y.value\n","\n","GRADS[math.exp] = exp_grad"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"23F_ZDdegRZi"},"cell_type":"markdown","source":["**Unit tests**: run the following cell to check your implementation."]},{"metadata":{"colab_type":"code","cellView":"form","outputId":"35b11264-9405-4cbb-bc6c-e926ac135fe6","id":"IJetYFiTgRZk","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1552989940030,"user_tz":-120,"elapsed":1778,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["#@title\n","class TestOperatorGrads(unittest.TestCase):\n","  def test_mul_grad(self):\n","    def _f(x, y):\n","      return x * y\n","    dx, dy = grad(_f)(Float(2), Float(3))\n","    self.assertAlmostEqual(dx, 3.0)\n","    self.assertAlmostEqual(dy, 2.0)\n","\n","  def test_truediv_grad(self):\n","    def _f(x, y):\n","      return x / y\n","    dx, dy = grad(_f)(Float(2), Float(3))\n","    self.assertAlmostEqual(dx, 1.0 / 3.0)\n","    self.assertAlmostEqual(dy, -2.0 / 9.0)\n","\n","  def test_neg_grad(self):\n","    def _f(x):\n","      return -x\n","    dx, = grad(_f)(Float(2))\n","    self.assertAlmostEqual(dx, -1.0)\n","\n","  def test_exp_grad(self):\n","    def _f(x):\n","      return exp(x)\n","    dx, = grad(_f)(Float(2))\n","    self.assertAlmostEqual(dx, math.exp(2.0))\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestOperatorGrads)\n","_ = unittest.TextTestRunner(verbosity=2).run(suite)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["test_exp_grad (__main__.TestOperatorGrads) ... ok\n","test_mul_grad (__main__.TestOperatorGrads) ... ok\n","test_neg_grad (__main__.TestOperatorGrads) ... ok\n","test_truediv_grad (__main__.TestOperatorGrads) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 4 tests in 0.010s\n","\n","OK\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"nZ0tqh0Fh9wH"},"cell_type":"markdown","source":["After you implemented these gradients, we can calculate the derivative of both $x$ and $t$ in one pass. Let's try our validated implementation on a simple example."]},{"metadata":{"colab_type":"code","outputId":"33e74d03-023c-4da6-c3fb-0a94280eaa73","id":"_WF8mnBKnKeE","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552989942580,"user_tz":-120,"elapsed":1091,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["# We can calculate the gradients of all the arguments.\n","dx, dt, _ = grad(logistic)(Float(1), Float(2), Float(1))\n","print(f\"dx = {dx:.4}, dt = {dt:.4}\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["dx = 0.21, dt = 0.105\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"UTH5y3NtnKeE"},"cell_type":"markdown","source":["#### Exercise\n","\n","Check to see if your code runs and returns correct gradients using finite differences."]},{"metadata":{"colab_type":"code","outputId":"2c8ee285-3e17-4068-ab33-a8f59fdd5212","id":"iB0VdfmZnKeH","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552990052251,"user_tz":-120,"elapsed":1123,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["# We can compare the gradient dx against the forward mode (0.21)\n","# Let's also check whether dt is correct\n","Delta = 0.01\n","dt = (logistic(x.a , t.a + Delta, one.a)-logistic(x.a , t.a - Delta, one.a))/(2*Delta) # Replace this with the finite difference approximation\n","\n","print(f\"correct dt = {dt:.4}\")"],"execution_count":38,"outputs":[{"output_type":"stream","text":["correct dt = 0.105\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"mYqEX7HdnKeM"},"cell_type":"markdown","source":["# Bonus: Higher-order derivatives\n","\n","A common problem with AD implementations is that higher-order derivatives are not supported. Operator overloading is actually one of the easiest cases to implement higher-order derivatives for, so let's see what we need to change to make AD closed under its own operation.\n","\n","The main problem is that higher-order derivatives require us to write the gradient calculations to the tape as well. Right now, the gradients are just numbers (e.g. `self.grad = 0` instead of `self.grad = Float(0.0)`. Setting `self.grad = Float(0.0)` creates an infinite loop though, so we need to be a bit more clever.)\n","\n","Also, in all the places where we assumed that the `grad` attribute was a number, we need to replace it with an instance of `Float`."]},{"metadata":{"colab_type":"code","id":"K7_ABBrJnKeN","colab":{}},"cell_type":"code","source":["class Float:\n","  def __init__(self, value):\n","      self.value = value\n","      self._grad = None\n","\n","  @property\n","  def grad(self):\n","    if self._grad is None:\n","      self._grad = Float(0.0)\n","    return self._grad\n","\n","  @grad.setter\n","  def grad(self, value):\n","    self._grad = value\n","\n","  # Overloaded operators\n","  __add__ = trace(operator.add)\n","  __truediv__ = trace(operator.truediv)\n","  __mul__ = trace(operator.mul)\n","  __neg__ = trace(operator.neg)\n","\n","  def __repr__(self):\n","    return repr(self.value)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"jEkfd6WBnKeP","colab":{}},"cell_type":"code","source":["def mul_grad(x, y, z):\n","  x.grad += y * z.grad\n","  y.grad += x * z.grad\n","\n","GRADS[operator.mul] = mul_grad\n","\n","def truediv_grad(x, y, z):\n","  x.grad += z.grad / y\n","  y.grad += -z.grad * x / (y * y)\n","\n","GRADS[operator.truediv] = truediv_grad\n","\n","def neg_grad(x, y):\n","  x.grad += -y.grad\n","\n","GRADS[operator.neg] = neg_grad\n","\n","def exp_grad(x, y):\n","  x.grad += y.grad * y\n","\n","GRADS[math.exp] = exp_grad"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"KqzUfMZGnKeQ","colab":{}},"cell_type":"code","source":["def grad(f):\n","  def df(*args, **kwargs):\n","    # Now call the original function, which will write to the tape\n","    out = f(*args, **kwargs)\n","\n","    # The initial gradient of the output is 1\n","    out._grad = Float(1)\n","\n","    for entry in reversed(TAPE):\n","      grad_function = GRADS[entry.function]\n","      grad_function(*entry.inputs, entry.output)\n","\n","    # We return the gradient with respect to each of the input arguments\n","    return tuple(arg.grad if isinstance(arg, Float) else None for arg in args)\n","  return df\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","outputId":"148833d1-a4ab-45e3-93b6-443bc86e5035","id":"bRbhchl_nKeS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1552990072104,"user_tz":-120,"elapsed":1227,"user":{"displayName":"Belona Sonna","photoUrl":"https://lh3.googleusercontent.com/-yriJXGbtZMA/AAAAAAAAAAI/AAAAAAAAAA8/ZDbdkQav83Y/s64/photo.jpg","userId":"02486431766251900434"}}},"cell_type":"code","source":["def grad_logistic(*args):\n","  dx, dt, _ = grad(logistic)(*args)\n","  return dx\n","\n","# We can calculate the gradients of all the arguments.\n","TAPE.clear()\n","dxdx, dxdt, _ = grad(grad_logistic)(Float(1), Float(2), Float(1))\n","print(f\"dxdx = {dxdx.value:.4}, dxdt = {dxdt.value:.4}\")"],"execution_count":42,"outputs":[{"output_type":"stream","text":["dxdx = 0.9401, dxdt = 0.575\n"],"name":"stdout"}]},{"metadata":{"id":"FoOhbIoYZjCa","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}